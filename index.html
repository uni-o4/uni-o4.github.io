<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization">
  <meta name="keywords" content="Offline-to-Online Fine-tuning, On-policy Learning, Robot Learning, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" integrity="sha512-iBBXm8fW90+nuLcSKlbmrPcLa0OT92xO1BIsZ+ywDWZCvqsWgccV3gFoRBv0z+8dLJgyAHIhR35VZc2oM/gI1w==" crossorigin="anonymous" referrerpolicy="no-referrer" /> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
  <link rel="icon" href="./favicon.ico">

  <meta property="og:site_name" content="Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization" />
</head>


<body>

<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span>: <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni</span>fying <span style="color: rgb(128, 185, 90);font-weight: bolder;">O</span>nline and <span style="color: rgb(128, 185, 90);font-weight: bolder;">O</span>ffline Deep Reinforcement Learning with Multi-Step <span style="color: rgb(128, 185, 90);font-weight: bolder;">O</span>n-Policy <span style="color: rgb(128, 185, 90);font-weight: bolder;">O</span>ptimization</h1>
          <div class="column has-text-centered">
            <div class="publication-links">
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Combining offline and online reinforcement learning (RL) is crucial for efficient and safe learning. However, previous approaches treat offline and online learning as separate procedures,
             resulting in redundant designs and limited performance. We ask:
              <em>Can we achieve straightforward yet effective offline and online learning without introducing extra conservatism or regularization?</em>
              In this study, we propose <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span>, which utilizes an on-policy objective for both offline and online learning. Owning to the alignment of objectives in two phases,
               the RL agent can transfer between offline and online learning seamlessly. This property enhances the flexibility of the learning paradigm, allowing for arbitrary combinations of pretraining,
                fine-tuning, offline, and online learning. In the offline phase, specifically, Uni-O4 leverages diverse ensemble policies to address the mismatch issues between the estimated behavior policy and the offline dataset.
                 Through a simple offline policy evaluation (OPE) approach, <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> can achieve multi-step policy improvement safely. We demonstrate that by employing the method above, the fusion of these two paradigms can yield superior offline initialization as well as stable and rapid online fine-tuning capabilities.
                  Through real-world robot tasks, we highlight the benefits of this paradigm for rapid deployment in challenging, previously unseen real-world environments.
                   Additionally, through comprehensive evaluations using numerous simulated benchmarks, we substantiate that our method achieves state-of-the-art performance in both offline and offline-to-online fine-tuning learning. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
        <div class="column has-text-centered">
<!--       <div class="column is-two-thirds"> -->
        <h2 class="title is-2">Motivated Example</h2>
        <div class="content has-text-justified">
          <p>
            The objective of offline-to-online RL algorithms is to strike a trade-off between fine-tuning stability and asymptotical optimality.
            Challenges arise from the inherent conservatism during the offline stage and the difficulties associated with off-policy evaluation during the offline-to-online stage.
            To provide a clearer understanding, we track the average values of the <em>V</em> and <em>Q</em> functions during fine-tuning with three different methods. As depicted in the right Figure below,
            the <em>Q</em> values of the off-policy (SAC) and conservative algorithms exhibit instability and slow improvement, respectively.
          </p>
          <p>
            Hence, we raise the question: <em>Is it possible to avoid introducing the conservatism term during offline training and eliminate the need for off-policy evaluation during offline-to-online fine-tuning?</em> 
            At the heart of this paper is an <em><strong>on-policy optimization method that unifies both offline and online training without extra regularization</strong></em>, which we term <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span>.
            <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> presents steady and rapid improvement in the <em>V</em> values as the fine-tuning performance progresses, emerging as a favorable choice for achieving both stable and efficient fine-tuning.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-one-third">
            <figure class="image">
              <img src="resources/video/motivation_example.PNG" alt="First Image">
            <figcaption>Normalized returns during online fine-tuning</figcaption>
            </figure>
          </div>
          <div class="column is-one-third">
            <figure class="image">
              <img src="resources/video/average_Value.PNG" alt="Second Image">
              <figcaption>Average values during online fine-tuning</figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  
<section class="section">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
        <div class="column has-text-centered">
<!--       <div class="column is-two-thirds"> -->
        <h2 class="title is-2">Method</h2>
        <div class="content has-text-justified">
          <p>
                Owning to the alignment of objectives in offline and online phases, <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> Uni-O4 enables flexible combinations of pretraining, fine-tuning, offline learning, and online learning.
                In this work, our main focus lies on introducing three settings: pure offline, offline-to-online, and online-to-offline-to-online. In the offline-to-online setting, our framework comprises three stages:
             1) the supervised learning stage, 2) the multi-step policy improvement stage, and 3) the online fine-tuning stage, as illustrated in the figure below.
          </p>
          <p>
            <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> employs supervised learning to learn the components for initializing the subsequent phase. In offline multi-step optimization phase (middle),
            policies query AM-Q to determine whether to update the behavior policies after a certain number of training steps. For instance, AM-Q allows \(\pi^{2}\) to update its behavior policy but rejects the others.
            Subsequently, one policy is selected as the initialization for online fine-tuning. Specifically, \(OOS_{\mathcal{D}}\) indicates out-of-support of dataset.
          </p>
        </div>
        <div style="text-align: center;">
          <figure style="display: inline-block; max-width: 75%;">
            <img src="resources/video/pipeline.png" alt="Pipeline Image" style="max-width: 100%;">
            <figcaption>Pipeline of the offline-to-online setting for <span style="color: rgb(128, 185, 90); font-weight: bolder;">Uni-O4</span></figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>
  
<section class="section">
  <div class="container">
    <h2 class="title is-2" style="text-align: center;">Real-World Robot Tasks</h2>
    <p>
      Here, <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> showcases its ability to excel in real-world robot applications. Bridging the sim-to-real gap is a widely recognized challenge in robot learning. Previous studies tackled this issue by employing domain randomization, 
      which involves training the agent in multiple randomized environments simultaneously. However, this approach comes with computational overhead and poses challenges when applied to real-world environments that are difficult to model in simulators. 
      To address this issue, we propose to leverage <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> in an online-offline-online framework. The agent is initially pretrained in simulators (online), followed by fine-tuning on real-world robots (offline and online), as illustrated in the following pipeline.
    </br>
    </br>
    <!--        Our approach introduces a novel process of online pretraining in simulators,
      followed by offline and online fine-tuning on real-world robots. -->
      The distinction between this paradigm and the previously introduced offline-to-online setting is that the behavior policies are directly initialized from the simulator pretraining, rather than being estimated using BC.
      In the offline fine-tuning phase, specifically, we deploy the pretrained policy on real-world robots to collect datasets in more challenging environments. As a result of fine-tuning the offline dataset, the policy becomes capable of ruining at a low speed in these demanding environments.
      Subsequently, we proceed with online fine-tuning to achieve further performance improvement. Overall, offline fine-tuning proves the safety of real-world robot learning, while online learning undergoes policy improvement. This paradigm showcases sample-efficient fine-tuning and safe robot learning.
      Results are presented in the following videos.
    </p>
    </br>
    </br>
        <div style="text-align: center;">
          <figure style="display: inline-block; max-width: 75%;">
            <img src="resources/video/real_robot_pipeline.png" alt="Pipeline Image" style="max-width: 100%;">
            <figcaption>The workflow of <span style="color: rgb(128, 185, 90); font-weight: bolder;">Uni-O4</span> online-offline-online fine-tuning framework on real-world robots. </figcaption>
          </figure>
        </div>
    <br>
    <br>
      <p style="text-align: center;"><strong>Offline fine-tuned</strong> (0.18 million env steps collected) by <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> <em>VS.</em> <strong>simulator pretrained</strong> (10 minutes training) policy deployment with low speed <span style="font-size: 14px; text-shadow: 0 0 3px rgb(161, 66, 170);">&darr;</span></p>
      <div class="column is-full-width is-centered has-text-centered">
        <video controls autoplay loop muted playsinline src="./resources/video/Offline_fine_tuned_Sim_pretrained_720_compressed.mp4" poster="./resources/loading-icon.gif" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%; width: 70%"></video>
      </div>
      <p style="text-align: center;"><strong>Online fine-tuned</strong> (0.1 million env steps) by <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> <em>VS.</em> <strong>Offline fine-tuned</strong> by <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> policy deployment with high speed <span style="font-size: 14px; text-shadow: 0 0 3px rgb(161, 66, 170);">&darr;</span></p>
      <div class="column is-full-width is-centered has-text-centered">
        <video controls autoplay loop muted playsinline src="./resources/video/Online_Offline_fine_tuned_720_compressed.mp4" poster="./resources/loading-icon.gif" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%; width: 70%"></video>
      </div>
      <p style="text-align: center;"><strong>Online fine-tuned</strong> by <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> <em>VS.</em> <strong>IQL offline-to-online fine-tuned</strong> policy deployment with high speed <span style="font-size: 14px; text-shadow: 0 0 3px rgb(161, 66, 170);">&darr;</span></p>
      <div class="column is-full-width is-centered has-text-centered">
        <video controls autoplay loop muted playsinline src="./resources/video/Online_IQL_720_compressed.mp4" poster="./resources/loading-icon.gif" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%; width: 70%"></video>
      </div>
      <p style="text-align: center;"><strong>Online fine-tuned</strong> by <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> <em>VS.</em> <strong>sim2real baseline</strong> policy deployment with high speed <span style="font-size: 14px; text-shadow: 0 0 3px rgb(161, 66, 170);">&darr;</span></p>
      <div class="column is-full-width is-centered has-text-centered">
        <video controls autoplay loop muted playsinline src="./resources/video/Online_Sim2Real_720_compressed.mp4" poster="./resources/loading-icon.gif" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%; width: 70%"></video>
      </div>
  </div>

<section class="section">
    <div class="container">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
          <div class="column has-text-centered">
  <!--       <div class="column is-two-thirds"> -->
          <h2 class="title is-2">Simulated Tasks</h2>
          <div class="content has-text-justified">
            <p>
              For simulated tasks,<span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> exhibits an integration of stability, consistency, and efficiency, eclipsing all baseline methods with its unified training scheme.
              As shown in the figures below, <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> can provide a better initialization from offline compared with other baselines and then shows a stable and rapid online fine-tuning performance improvement.
              For more results, please refer to our paper.
              </p>
          </div>
          <div class="columns is-centered">
            <!-- 第一行 -->
            <div class="column is-full">
              <figure class="image">
                <img src="resources/video/head_lable.png" alt="First Image">
              </figure>
            </div>
          </div>
          <div class="columns is-centered">
            <!-- 第一行 -->
            <div class="column is-one-quarter">
              <figure class="image">
                <img src="resources/video/pen-human-v1.PNG" alt="Second Image">
              </figure>
            </div>
            <div class="column is-one-quarter">
              <figure class="image">
                <img src="resources/video/hammer-human-v1.PNG" alt="Second Image">
              </figure>
            </div>
            <div class="column is-one-quarter">
              <figure class="image">
                <img src="resources/video/door-human-v1.PNG" alt="Second Image">
              </figure>
            </div>
            <div class="column is-one-quarter">
              <figure class="image">
                <img src="resources/video/walker2d-medium-v2.PNG" alt="Second Image">
              </figure>
            </div>
            </div>
            <!-- 第二行 -->
            <div class="columns is-centered">
            <div class="column is-one-quarter">
              <figure class="image">
                <img src="resources/video/walker2d-medium-replay-v2.PNG" alt="Second Image">
              </figure>
            </div>
            <div class="column is-one-quarter">
              <figure class="image">
                <img src="resources/video/walker2d-medium-expert-v2.PNG" alt="Second Image">
              </figure>
            </div>
            <div class="column is-one-quarter">
              <figure class="image">
                <img src="resources/video/hopper-medium-v2.PNG" alt="Second Image">
              </figure>
            </div>
            <div class="column is-one-quarter">
              <figure class="image">
                <img src="resources/video/hopper-medium-replay-v2.PNG" alt="Second Image">
              </figure>
            </div>
            </div>
            <!-- 第三行 -->
            <div class="columns is-centered">
            <div class="column is-one-quarter">
              <figure class="image">
                <img src="resources/video/hopper-medium-expert-v2.PNG" alt="Second Image">
              </figure>
            </div>
            <div class="column is-one-quarter">
              <figure class="image">
                <img src="resources/video/halfcheetah-medium-v2.PNG" alt="Second Image">
              </figure>
            </div>
            <div class="column is-one-quarter">
              <figure class="image">
                <img src="resources/video/halfcheetah-medium-replay-v2.PNG" alt="Second Image">
              </figure>
            </div>
            <div class="column is-one-quarter">
              <figure class="image">
                <img src="resources/video/halfcheetah-medium-expert-v2.PNG" alt="Second Image">
              </figure>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


</body>
</html>
