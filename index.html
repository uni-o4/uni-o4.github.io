<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization">
  <meta name="keywords" content="Offline-to-Online Fine-tuning, On-policy Learning, Robot Learning, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" integrity="sha512-iBBXm8fW90+nuLcSKlbmrPcLa0OT92xO1BIsZ+ywDWZCvqsWgccV3gFoRBv0z+8dLJgyAHIhR35VZc2oM/gI1w==" crossorigin="anonymous" referrerpolicy="no-referrer" /> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
  <link rel="icon" href="./favicon.ico">

  <meta property="og:site_name" content="Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization" />
</head>


<body>

<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span>: <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni</span>fying <span style="color: rgb(128, 185, 90);font-weight: bolder;">O</span>nline and <span style="color: rgb(128, 185, 90);font-weight: bolder;">O</span>ffline Deep Reinforcement Learning with Multi-Step <span style="color: rgb(128, 185, 90);font-weight: bolder;">O</span>n-Policy <span style="color: rgb(128, 185, 90);font-weight: bolder;">O</span>ptimization</h1>
          <div class="column has-text-centered">
            <div class="publication-links">
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Combining offline and online reinforcement learning (RL) is crucial for efficient and safe learning. However, previous approaches treat offline and online learning as separate procedures,
             resulting in redundant designs and limited performance. We ask:
              <em>Can we achieve straightforward yet effective offline and online learning without introducing extra conservatism or regularization?</em>
              In this study, we propose <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span>, which utilizes an on-policy objective for both offline and online learning. Owning to the alignment of objectives in two phases,
               the RL agent can transfer between offline and online learning seamlessly. This property enhances the flexibility of the learning paradigm, allowing for arbitrary combinations of pretraining,
                fine-tuning, offline, and online learning. In the offline phase, specifically, Uni-O4 leverages diverse ensemble policies to address the mismatch issues between the estimated behavior policy and the offline dataset.
                 Through a simple offline policy evaluation (OPE) approach, <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> can achieve multi-step policy improvement safely. We demonstrate that by employing the method above, the fusion of these two paradigms can yield superior offline initialization as well as stable and rapid online fine-tuning capabilities.
                  Through real-world robot tasks, we highlight the benefits of this paradigm for rapid deployment in challenging, previously unseen real-world environments.
                   Additionally, through comprehensive evaluations using numerous simulated benchmarks, we substantiate that our method achieves state-of-the-art performance in both offline and offline-to-online fine-tuning learning. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
        <div class="column has-text-centered">
<!--       <div class="column is-two-thirds"> -->
        <h2 class="title is-2">Motivated Example</h2>
        <div class="content has-text-justified">
          <p>
            The objective of offline-to-online RL algorithms is to strike a trade-off between fine-tuning stability and asymptotical optimality.
            Challenges arise from the inherent conservatism during the offline stage and the difficulties associated with off-policy evaluation during the offline-to-online stage.
            To provide a clearer understanding, we track the average values of the <em>V</em> and <em>Q</em> functions during fine-tuning with three different methods. As depicted in the right Figure below,
            the <em>Q</em> values of the off-policy (SAC) and conservative algorithms exhibit instability and slow improvement, respectively.
            Hence, we raise the question: <em>Is it possible to avoid introducing the conservatism term during offline training and eliminate the need for off-policy evaluation during offline-to-online fine-tuning?</em> 
          </p>
          <p>
            At the heart of this paper is an <em><strong>on-policy optimization method that unifies both offline and online training without extra regularization</strong></em>, which we term <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span>.
            <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> presents steady and rapid improvement in the <em>V</em> values as the fine-tuning performance progresses, emerging as a favorable choice for achieving both stable and efficient fine-tuning.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-one-third">
            <figure class="image">
              <img src="resources/video/motivation_example.PNG" alt="First Image">
            <figcaption>Normalized returns during online fine-tuning</figcaption>
            </figure>
          </div>
          <div class="column is-one-third">
            <figure class="image">
              <img src="resources/video/average_Value.PNG" alt="Second Image">
              <figcaption>Average values during online fine-tuning</figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  
<section class="section">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
        <div class="column has-text-centered">
<!--       <div class="column is-two-thirds"> -->
        <h2 class="title is-2">Method</h2>
        <div class="content has-text-justified">
          <p>
                Owning to the alignment of objectives in offline and online phases, <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> Uni-O4 enables flexible combinations of pretraining, fine-tuning, offline learning, and online learning.
                In this work, our main focus lies on introducing three settings: pure offline, offline-to-online, and online-to-offline-to-online. In the offline-to-online setting, our framework comprises three stages:
             1) the supervised learning stage, 2) the multi-step policy improvement stage, and 3) the online fine-tuning stage, as illustrated in the figure below.
          </p>
          <p>
            <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> employs supervised learning to learn the components for initializing the subsequent phase. In offline multi-step optimization phase (middle),
            policies query AM-Q to determine whether to update the behavior policies after a certain number of training steps. For instance, AM-Q allows $\pi^{2}$ to update its behavior policy but rejects the others.
            Subsequently, one policy is selected as the initialization for online fine-tuning. Specifically, $OOS_{\mathcal{D}}$ indicates out-of-support of dataset.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-two-third">
            <figure class="image">
              <img src="resources/video/pipeline.png" alt="Second Image">
              <figcaption>Pipeline of offline-to-online setting for <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span></figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  
<section class="section">
  <div class="container">
    <h2 class="title is-2" style="text-align: center;">Real-World Robot Tasks</h2>
      <p style="text-align: center;"><strong>Offline fine-tuned</strong> (0.18 million env steps collected) by <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> <em>VS.</em> <strong>simulator pretrained</strong> (10 minutes training) policy deployment with low speed <span style="font-size: 14px; text-shadow: 0 0 3px rgb(161, 66, 170);">&darr;</span></p>
      <div class="column is-full-width is-centered has-text-centered">
        <video controls autoplay loop muted playsinline src="./resources/video/Offline_fine_tuned_Sim_pretrained_720_compressed.mp4" poster="./resources/loading-icon.gif" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%; width: 70%"></video>
      </div>
      <p style="text-align: center;"><strong>Online fine-tuned</strong> (0.1 million env steps) by <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> <em>VS.</em> <strong>Offline fine-tuned</strong> by <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> policy deployment with high speed <span style="font-size: 14px; text-shadow: 0 0 3px rgb(161, 66, 170);">&darr;</span></p>
      <div class="column is-full-width is-centered has-text-centered">
        <video controls autoplay loop muted playsinline src="./resources/video/Online_Offline_fine_tuned_720_compressed.mp4" poster="./resources/loading-icon.gif" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%; width: 70%"></video>
      </div>
      <p style="text-align: center;"><strong>Online fine-tuned</strong> by <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> <em>VS.</em> <strong>IQL offline-to-online fine-tuned</strong> policy deployment with high speed <span style="font-size: 14px; text-shadow: 0 0 3px rgb(161, 66, 170);">&darr;</span></p>
      <div class="column is-full-width is-centered has-text-centered">
        <video controls autoplay loop muted playsinline src="./resources/video/Online_IQL_720_compressed.mp4" poster="./resources/loading-icon.gif" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%; width: 70%"></video>
      </div>
      <p style="text-align: center;"><strong>Online fine-tuned</strong> by <span style="color: rgb(128, 185, 90);font-weight: bolder;">Uni-O4</span> <em>VS.</em> <strong>sim2real baseline</strong> policy deployment with high speed <span style="font-size: 14px; text-shadow: 0 0 3px rgb(161, 66, 170);">&darr;</span></p>
      <div class="column is-full-width is-centered has-text-centered">
        <video controls autoplay loop muted playsinline src="./resources/video/Online_Sim2Real_720_compressed.mp4" poster="./resources/loading-icon.gif" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%; width: 70%"></video>
      </div>
  </div>



</body>
</html>
